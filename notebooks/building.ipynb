{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "49c65769",
            "metadata": {},
            "source": [
                "# Building Footprint Extraction Setup\n",
                "\n",
                "This notebook sets up the data processing pipeline for loading high-resolution drone orthophotos and building shapefiles, ensuring proper alignment via reprojection and rasterization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "1738f77c",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import rasterio\n",
                "import geopandas as gpd\n",
                "import matplotlib.pyplot as plt\n",
                "from rasterio.features import rasterize\n",
                "from rasterio.windows import Window\n",
                "from shapely.geometry import box\n",
                "import numpy as np\n",
                "\n",
                "# Configure plots size\n",
                "plt.rcParams['figure.figsize'] = (10, 10)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "d6f61efd",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Image Path: ../data/images/28996_NADALA_ORTHO.tif\n",
                        "Shapefile Path: ../data/shp/Built_Up_Area_typ.shp\n"
                    ]
                }
            ],
            "source": [
                "# Paths\n",
                "# Using '28996_NADALA_ORTHO.tif' as the base orthophoto.\n",
                "IMAGE_PATH = \"../data/images/28996_NADALA_ORTHO.tif\"\n",
                "SHP_PATH = \"../data/shp/Built_Up_Area_typ.shp\"\n",
                "\n",
                "print(f\"Image Path: {IMAGE_PATH}\")\n",
                "print(f\"Shapefile Path: {SHP_PATH}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "8ff0c7db",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Orthophoto CRS: EPSG:32643\n",
                        "Orthophoto Shape: (27390, 26259)\n",
                        "Orthophoto Bounds: BoundingBox(left=540378.3252300001, bottom=3489958.00821, right=541176.59883, top=3490790.66421)\n"
                    ]
                }
            ],
            "source": [
                "# 1. Load Orthophoto Metadata\n",
                "with rasterio.open(IMAGE_PATH) as src:\n",
                "    ortho_meta = src.meta.copy()\n",
                "    ortho_crs = src.crs\n",
                "    ortho_transform = src.transform\n",
                "    ortho_shape = src.shape\n",
                "    ortho_bounds = src.bounds\n",
                "    \n",
                "    print(f\"Orthophoto CRS: {ortho_crs}\")\n",
                "    print(f\"Orthophoto Shape: {ortho_shape}\")\n",
                "    print(f\"Orthophoto Bounds: {ortho_bounds}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "301d758a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Original Shapefile CRS: EPSG:32643\n",
                        "CRS matches, no reprojection needed.\n",
                        "Shapefile bounds (reprojected): [ 508512.4309     3488140.2227      570932.38325358 3568121.8899    ]\n"
                    ]
                }
            ],
            "source": [
                "# 2. Load and Reproject Shapefile\n",
                "gdf = gpd.read_file(SHP_PATH)\n",
                "print(f\"Original Shapefile CRS: {gdf.crs}\")\n",
                "\n",
                "if gdf.crs != ortho_crs:\n",
                "    print(f\"Reprojecting shapefile from {gdf.crs} to {ortho_crs}...\")\n",
                "    gdf = gdf.to_crs(ortho_crs)\n",
                "    print(\"Reprojection complete.\")\n",
                "else:\n",
                "    print(\"CRS matches, no reprojection needed.\")\n",
                "\n",
                "# Check bounds intersection\n",
                "shp_bounds = gdf.total_bounds\n",
                "print(f\"Shapefile bounds (reprojected): {shp_bounds}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "e2207d96",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Windowed Rasterization and Verification Function\n",
                "def visualize_window(image_path, gdf, window_slice, title=\"Alignment Check\"):\n",
                "    \"\"\"\n",
                "    Reads a window from the large image, rasterizes the vector mask for that window, and plots them.\n",
                "    window_slice: Window(col_off, row_off, width, height)\n",
                "    \"\"\"\n",
                "    with rasterio.open(image_path) as src:\n",
                "        # Read the image data in the window\n",
                "        img_data = src.read(window=window_slice)\n",
                "        \n",
                "        # Calculate the transform for this specific window\n",
                "        window_transform = src.window_transform(window_slice)\n",
                "        \n",
                "        # Get the spatial bounds of the window to filter polygons\n",
                "        bounds = rasterio.windows.bounds(window_slice, src.transform)\n",
                "        \n",
                "        # Filter visible polygons using spatial index\n",
                "        visible_polys = gdf.cx[bounds[0]:bounds[2], bounds[1]:bounds[3]]\n",
                "        \n",
                "        if visible_polys.empty:\n",
                "            print(\"No buildings found in this window.\")\n",
                "            mask = np.zeros((window_slice.height, window_slice.width), dtype='uint8')\n",
                "        else:\n",
                "            # Rasterize relevant polygons\n",
                "            shapes = ((geom, 1) for geom in visible_polys.geometry)\n",
                "            mask = rasterize(\n",
                "                shapes,\n",
                "                out_shape=(window_slice.height, window_slice.width),\n",
                "                transform=window_transform,\n",
                "                fill=0,\n",
                "                default_value=1,\n",
                "                dtype='uint8'\n",
                "            )\n",
                "            \n",
                "    # Plotting\n",
                "    fig, ax = plt.subplots(1, 3, figsize=(20, 8))\n",
                "    \n",
                "    # Prepare RGB image for display\n",
                "    if img_data.shape[0] >= 3:\n",
                "        rgb_img = np.transpose(img_data[:3, :, :], (1, 2, 0))\n",
                "    else:\n",
                "        rgb_img = img_data[0, :, :]\n",
                "        \n",
                "    # Normalize if necessary (simple min-max for display)\n",
                "    if rgb_img.dtype != np.uint8:\n",
                "        rgb_img = ((rgb_img - rgb_img.min()) / (rgb_img.max() - rgb_img.min()) * 255).astype(np.uint8)\n",
                "\n",
                "    ax[0].imshow(rgb_img)\n",
                "    ax[0].set_title(\"Orthophoto Window\")\n",
                "    ax[0].axis('off')\n",
                "    \n",
                "    ax[1].imshow(mask, cmap='gray', interpolation='nearest')\n",
                "    ax[1].set_title(\"Rasterized Mask\")\n",
                "    ax[1].axis('off')\n",
                "    \n",
                "    # Overlay\n",
                "    ax[2].imshow(rgb_img)\n",
                "    ax[2].imshow(mask, alpha=0.5, cmap='spring')  # Overlay mask with transparency\n",
                "    ax[2].set_title(\"Overlay\")\n",
                "    ax[2].axis('off')\n",
                "    \n",
                "    plt.suptitle(title)\n",
                "    plt.tight_layout()\n",
                "    plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "05c1d11e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Displaying window: Window(col_off=0, row_off=0, width=2000, height=2000) around center (539722.4070767914, 3528131.0563000003)\n",
                        "No buildings found in this window.\n"
                    ]
                },
                {
                    "data": {
                        "image/png": "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",
                        "text/plain": [
                            "<Figure size 2000x800 with 3 Axes>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# 4. Select a Window and Visualize\n",
                "# We'll attempt to find a center point from the shapefile bounds and sample there.\n",
                "\n",
                "minx, miny, maxx, maxy = gdf.total_bounds\n",
                "center_x = (minx + maxx) / 2\n",
                "center_y = (miny + maxy) / 2\n",
                "\n",
                "with rasterio.open(IMAGE_PATH) as src:\n",
                "    # Convert map coordinates to pixel coordinates\n",
                "    row, col = src.index(center_x, center_y)\n",
                "    \n",
                "    # Define window size\n",
                "    w_width, w_height = 2000, 2000\n",
                "    \n",
                "    # Calculate window offset\n",
                "    col_off = max(0, col - w_width // 2)\n",
                "    row_off = max(0, row - w_height // 2)\n",
                "    \n",
                "    # Create Window object\n",
                "    window = Window(col_off, row_off, w_width, w_height)\n",
                "    \n",
                "    print(f\"Displaying window: {window} around center ({center_x}, {center_y})\")\n",
                "    visualize_window(IMAGE_PATH, gdf, window)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "2428b240",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Output directories ready:\n",
                        "Images: ../data/tiles/images\n",
                        "Masks: ../data/tiles/masks\n",
                        "Processing image (26259x27390) into 1024x1024 tiles...\n",
                        "Total grid cells to check: 702\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Generating Tiles:  52%|█████▏    | 362/702 [01:18<01:51,  3.04it/s]/home/anushtup-ghosh/Documents/Projects/natgeo/venv/lib/python3.13/site-packages/rasterio/features.py:336: ShapeSkipWarning: Invalid or empty shape {'type': 'Polygon', 'coordinates': (((540794.1804244192, 3490438.244493237), (540794.1126661297, 3490438.2830306366), (540794.1804244192, 3490438.244493237)),)} at index 20 will not be rasterized.\n",
                        "  warnings.warn(\n",
                        "Generating Tiles: 100%|██████████| 702/702 [02:28<00:00,  4.72it/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Processing Complete.\n",
                        "Saved 429 tiles to ../data/tiles\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 5. Tiling Pipeline implementation\n",
                "import os\n",
                "import numpy as np\n",
                "from tqdm import tqdm  # Changed from tqdm.notebook to standard tqdm to avoid IProgress error\n",
                "import rasterio\n",
                "from rasterio.windows import Window\n",
                "from rasterio.features import rasterize\n",
                "\n",
                "# Constants\n",
                "TILE_SIZE = 1024\n",
                "OUTPUT_DIR = \"../data/tiles\"\n",
                "IMG_DIR = os.path.join(OUTPUT_DIR, \"images\")\n",
                "MASK_DIR = os.path.join(OUTPUT_DIR, \"masks\")\n",
                "\n",
                "# Create directories\n",
                "os.makedirs(IMG_DIR, exist_ok=True)\n",
                "os.makedirs(MASK_DIR, exist_ok=True)\n",
                "print(f\"Output directories ready:\\nImages: {IMG_DIR}\\nMasks: {MASK_DIR}\")\n",
                "\n",
                "def process_tiles(image_path, gdf, tile_size=1024):\n",
                "    \"\"\"\n",
                "    Iterates over the orthophoto in fixed-size windows.\n",
                "    Checks for building overlap using the vector data.\n",
                "    Saves RGB crops and corresponding rasterized binary masks.\n",
                "    \"\"\"\n",
                "    with rasterio.open(image_path) as src:\n",
                "        width = src.width\n",
                "        height = src.height\n",
                "        \n",
                "        # Calculate total tiles for progress bar\n",
                "        cols = range(0, width, tile_size)\n",
                "        rows = range(0, height, tile_size)\n",
                "        total_tiles = len(cols) * len(rows)\n",
                "        \n",
                "        print(f\"Processing image ({width}x{height}) into {tile_size}x{tile_size} tiles...\")\n",
                "        print(f\"Total grid cells to check: {total_tiles}\")\n",
                "        \n",
                "        saved_count = 0\n",
                "        \n",
                "        # Base metadata for output tiles\n",
                "        meta = src.meta.copy()\n",
                "        meta.update({\n",
                "            \"driver\": \"GTiff\",\n",
                "            \"height\": tile_size,\n",
                "            \"width\": tile_size,\n",
                "            \"count\": 3,  # Force RGB\n",
                "            \"dtype\": \"uint8\"\n",
                "        })\n",
                "        \n",
                "        mask_meta = meta.copy()\n",
                "        mask_meta.update({\"count\": 1})  # Single band mask\n",
                "        \n",
                "        # Use tqdm for progress\n",
                "        pbar = tqdm(total=total_tiles, desc=\"Generating Tiles\")\n",
                "        \n",
                "        for col_off in cols:\n",
                "            for row_off in rows:\n",
                "                # Define window\n",
                "                window = Window(col_off, row_off, tile_size, tile_size)\n",
                "                \n",
                "                # 1. Fast check: Use vector spatial index to see if any buildings intersect this window bounds\n",
                "                # Get spatial bounds of the window\n",
                "                win_bounds = rasterio.windows.bounds(window, src.transform)\n",
                "                \n",
                "                # Query spatial index\n",
                "                possible_matches = gdf.cx[win_bounds[0]:win_bounds[2], win_bounds[1]:win_bounds[3]]\n",
                "                \n",
                "                if possible_matches.empty:\n",
                "                    pbar.update(1)\n",
                "                    continue\n",
                "                \n",
                "                # 2. Detailed processing: Read image and rasterize mask\n",
                "                # boundless=True automatically pads tiles at edges with fill_value=0\n",
                "                img_data = src.read(window=window, boundless=True, fill_value=0)\n",
                "                \n",
                "                # Ensure we strictly have 3 bands (RGB) - drop Alpha if present\n",
                "                if img_data.shape[0] >= 3:\n",
                "                    rgb_img = img_data[:3, :, :]\n",
                "                else:\n",
                "                    # Handle single band case if necessary (not expected for ortho)\n",
                "                    pbar.update(1)\n",
                "                    continue\n",
                "                \n",
                "                # Normalize/Cast to uint8\n",
                "                if rgb_img.dtype != np.uint8:\n",
                "                    # Assuming standard range or 16-bit, normalize to 0-255\n",
                "                    # A safe heuristic if max > 255 is to scale, otherwise just cast\n",
                "                    if rgb_img.max() > 255:\n",
                "                         rgb_img = (rgb_img / 255).astype('uint8')\n",
                "                    else:\n",
                "                         rgb_img = rgb_img.astype('uint8')\n",
                "\n",
                "                # Rasterize Mask\n",
                "                # Get window transform\n",
                "                win_transform = src.window_transform(window)\n",
                "                \n",
                "                shapes = ((geom, 1) for geom in possible_matches.geometry)\n",
                "                \n",
                "                mask = rasterize(\n",
                "                    shapes,\n",
                "                    out_shape=(tile_size, tile_size),\n",
                "                    transform=win_transform,\n",
                "                    fill=0,\n",
                "                    default_value=1,\n",
                "                    dtype='uint8'\n",
                "                )\n",
                "                \n",
                "                # 3. Final Check: is the mask actually containing pixels? (removes empty overlaps)\n",
                "                if mask.max() == 0:\n",
                "                    pbar.update(1)\n",
                "                    continue\n",
                "                \n",
                "                # 4. Save to Disk\n",
                "                tile_id = f\"tile_{col_off}_{row_off}.tif\"\n",
                "                \n",
                "                # Update metadata with specific transform for this tile\n",
                "                meta.update({\"transform\": win_transform})\n",
                "                mask_meta.update({\"transform\": win_transform})\n",
                "                \n",
                "                # Write Image\n",
                "                with rasterio.open(os.path.join(IMG_DIR, tile_id), 'w', **meta) as dst:\n",
                "                    dst.write(rgb_img)\n",
                "                    \n",
                "                # Write Mask (must be 3D array (1, H, W) for rasterio check or consistent with count=1)\n",
                "                with rasterio.open(os.path.join(MASK_DIR, tile_id), 'w', **mask_meta) as dst:\n",
                "                    dst.write(mask, 1)\n",
                "                \n",
                "                saved_count += 1\n",
                "                pbar.update(1)\n",
                "        \n",
                "        pbar.close()\n",
                "        print(f\"\\nProcessing Complete.\")\n",
                "        print(f\"Saved {saved_count} tiles to {OUTPUT_DIR}\")\n",
                "\n",
                "# Run the pipeline\n",
                "process_tiles(IMAGE_PATH, gdf, TILE_SIZE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "aea16463",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/anushtup-ghosh/Documents/Projects/natgeo/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Using device: cuda\n",
                        "Model initialized and moved to device.\n",
                        "Loss function: BCE + Dice Loss initialized.\n",
                        "Metrics function defined (IoU, Accuracy).\n"
                    ]
                }
            ],
            "source": [
                "# 6. Deep Learning Model Setup\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import segmentation_models_pytorch as smp\n",
                "\n",
                "# Check for CUDA\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")\n",
                "\n",
                "# 6.1 Model Definition\n",
                "# UNet with ResNet34 backbone, pretrained on ImageNet\n",
                "model = smp.Unet(\n",
                "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
                "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
                "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
                "    classes=1,                      # model output channels (number of classes in your dataset)\n",
                ")\n",
                "\n",
                "model.to(DEVICE)\n",
                "print(\"Model initialized and moved to device.\")\n",
                "\n",
                "# 6.2 Loss Function\n",
                "# Combination of BCE and Dice Loss for robust training with class imbalance\n",
                "class BCEDiceLoss(nn.Module):\n",
                "    def __init__(self, weight=None, size_average=True):\n",
                "        super(BCEDiceLoss, self).__init__()\n",
                "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
                "        self.dice_loss = smp.losses.DiceLoss(mode='binary', from_logits=True)\n",
                "\n",
                "    def forward(self, pred, target):\n",
                "        # BCE\n",
                "        bce = self.bce_loss(pred, target)\n",
                "        # Dice\n",
                "        dice = self.dice_loss(pred, target)\n",
                "        \n",
                "        return bce + dice\n",
                "\n",
                "loss_fn = BCEDiceLoss()\n",
                "print(\"Loss function: BCE + Dice Loss initialized.\")\n",
                "\n",
                "# 6.3 Metrics\n",
                "# IoU (Jaccard Index) and Accuracy\n",
                "def calculate_metrics(pred_mask, true_mask, threshold=0.5):\n",
                "    \"\"\"\n",
                "    Calculates IoU and Accuracy for binary segmentation.\n",
                "    pred_mask: logits from model (B, 1, H, W)\n",
                "    true_mask: ground truth (B, 1, H, W)\n",
                "    \"\"\"\n",
                "    # Apply sigmoid and threshold\n",
                "    pred_mask = (torch.sigmoid(pred_mask) > threshold).float()\n",
                "    \n",
                "    # Flatten\n",
                "    pred_flat = pred_mask.view(-1)\n",
                "    true_flat = true_mask.view(-1)\n",
                "    \n",
                "    # IoU\n",
                "    intersection = (pred_flat * true_flat).sum()\n",
                "    union = pred_flat.sum() + true_flat.sum() - intersection\n",
                "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
                "    \n",
                "    # Accuracy\n",
                "    correct = (pred_flat == true_flat).sum()\n",
                "    accuracy = correct.float() / true_flat.numel()\n",
                "    \n",
                "    return iou.item(), accuracy.item()\n",
                "\n",
                "print(\"Metrics function defined (IoU, Accuracy).\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "661a2cee",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/tmp/ipykernel_28881/2607385786.py:94: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
                        "  scaler = GradScaler() # Initialize Scaler for AMP\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total tiles: 429\n",
                        "Training: 343, Validation: 86\n",
                        "Starting training with AMP...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1/30:   0%|          | 0/86 [00:00<?, ?it/s]/tmp/ipykernel_28881/2607385786.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with autocast():\n",
                        "Epoch 1/30: 100%|██████████| 86/86 [00:13<00:00,  6.24it/s, iou=0.614, loss=0.689]\n",
                        "/tmp/ipykernel_28881/2607385786.py:145: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
                        "  with autocast():\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 1 Summary:\n",
                        "Train Loss: 0.8215 | Train IoU: 0.6134\n",
                        "Val Loss: 0.5607 | Val IoU: 0.7544 | Val Acc: 0.8553\n",
                        "Saved Best Model! (IoU: 0.7544)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 2/30: 100%|██████████| 86/86 [00:12<00:00,  6.97it/s, iou=0.802, loss=0.456]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 2 Summary:\n",
                        "Train Loss: 0.5925 | Train IoU: 0.7206\n",
                        "Val Loss: 0.5021 | Val IoU: 0.7766 | Val Acc: 0.8733\n",
                        "Saved Best Model! (IoU: 0.7766)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 3/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.868, loss=0.329]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 3 Summary:\n",
                        "Train Loss: 0.5617 | Train IoU: 0.7285\n",
                        "Val Loss: 0.4516 | Val IoU: 0.8031 | Val Acc: 0.8962\n",
                        "Saved Best Model! (IoU: 0.8031)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 4/30: 100%|██████████| 86/86 [00:12<00:00,  7.01it/s, iou=0.843, loss=0.347]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 4 Summary:\n",
                        "Train Loss: 0.4578 | Train IoU: 0.7852\n",
                        "Val Loss: 0.4236 | Val IoU: 0.8107 | Val Acc: 0.8947\n",
                        "Saved Best Model! (IoU: 0.8107)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 5/30: 100%|██████████| 86/86 [00:12<00:00,  6.94it/s, iou=0.819, loss=0.358]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 5 Summary:\n",
                        "Train Loss: 0.3967 | Train IoU: 0.8136\n",
                        "Val Loss: 0.3738 | Val IoU: 0.8282 | Val Acc: 0.9087\n",
                        "Saved Best Model! (IoU: 0.8282)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 6/30: 100%|██████████| 86/86 [00:12<00:00,  6.89it/s, iou=0.687, loss=0.545]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 6 Summary:\n",
                        "Train Loss: 0.3510 | Train IoU: 0.8336\n",
                        "Val Loss: 0.3496 | Val IoU: 0.8399 | Val Acc: 0.9185\n",
                        "Saved Best Model! (IoU: 0.8399)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 7/30: 100%|██████████| 86/86 [00:12<00:00,  6.98it/s, iou=0.793, loss=0.387]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 7 Summary:\n",
                        "Train Loss: 0.3507 | Train IoU: 0.8322\n",
                        "Val Loss: 0.4088 | Val IoU: 0.8134 | Val Acc: 0.8983\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 8/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.787, loss=0.36] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 8 Summary:\n",
                        "Train Loss: 0.3203 | Train IoU: 0.8505\n",
                        "Val Loss: 0.3361 | Val IoU: 0.8449 | Val Acc: 0.9187\n",
                        "Saved Best Model! (IoU: 0.8449)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 9/30: 100%|██████████| 86/86 [00:12<00:00,  6.91it/s, iou=0.906, loss=0.241]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 9 Summary:\n",
                        "Train Loss: 0.3265 | Train IoU: 0.8436\n",
                        "Val Loss: 0.3089 | Val IoU: 0.8532 | Val Acc: 0.9254\n",
                        "Saved Best Model! (IoU: 0.8532)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 10/30: 100%|██████████| 86/86 [00:12<00:00,  7.00it/s, iou=0.906, loss=0.231]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 10 Summary:\n",
                        "Train Loss: 0.2857 | Train IoU: 0.8640\n",
                        "Val Loss: 0.3344 | Val IoU: 0.8402 | Val Acc: 0.9188\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 11/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.823, loss=0.407]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 11 Summary:\n",
                        "Train Loss: 0.2465 | Train IoU: 0.8804\n",
                        "Val Loss: 0.3112 | Val IoU: 0.8534 | Val Acc: 0.9243\n",
                        "Saved Best Model! (IoU: 0.8534)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 12/30: 100%|██████████| 86/86 [00:12<00:00,  6.89it/s, iou=0.73, loss=0.413] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 12 Summary:\n",
                        "Train Loss: 0.2469 | Train IoU: 0.8744\n",
                        "Val Loss: 0.2878 | Val IoU: 0.8592 | Val Acc: 0.9285\n",
                        "Saved Best Model! (IoU: 0.8592)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 13/30: 100%|██████████| 86/86 [00:12<00:00,  6.96it/s, iou=0.914, loss=0.271]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 13 Summary:\n",
                        "Train Loss: 0.2335 | Train IoU: 0.8865\n",
                        "Val Loss: 0.2952 | Val IoU: 0.8572 | Val Acc: 0.9283\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 14/30: 100%|██████████| 86/86 [00:12<00:00,  6.96it/s, iou=0.482, loss=0.687]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 14 Summary:\n",
                        "Train Loss: 0.1907 | Train IoU: 0.9044\n",
                        "Val Loss: 0.2910 | Val IoU: 0.8584 | Val Acc: 0.9304\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 15/30: 100%|██████████| 86/86 [00:12<00:00,  6.91it/s, iou=0.933, loss=0.15] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 15 Summary:\n",
                        "Train Loss: 0.3031 | Train IoU: 0.8437\n",
                        "Val Loss: 0.3253 | Val IoU: 0.8462 | Val Acc: 0.9228\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 16/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.896, loss=0.229] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 16 Summary:\n",
                        "Train Loss: 0.2193 | Train IoU: 0.8891\n",
                        "Val Loss: 0.3383 | Val IoU: 0.8294 | Val Acc: 0.9188\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 17/30: 100%|██████████| 86/86 [00:12<00:00,  6.95it/s, iou=0.942, loss=0.134] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 17 Summary:\n",
                        "Train Loss: 0.1799 | Train IoU: 0.9098\n",
                        "Val Loss: 0.2804 | Val IoU: 0.8661 | Val Acc: 0.9302\n",
                        "Saved Best Model! (IoU: 0.8661)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 18/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.926, loss=0.131] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 18 Summary:\n",
                        "Train Loss: 0.1712 | Train IoU: 0.9173\n",
                        "Val Loss: 0.2749 | Val IoU: 0.8684 | Val Acc: 0.9321\n",
                        "Saved Best Model! (IoU: 0.8684)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 19/30: 100%|██████████| 86/86 [00:12<00:00,  6.94it/s, iou=0.948, loss=0.0981]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 19 Summary:\n",
                        "Train Loss: 0.1633 | Train IoU: 0.9192\n",
                        "Val Loss: 0.2615 | Val IoU: 0.8732 | Val Acc: 0.9357\n",
                        "Saved Best Model! (IoU: 0.8732)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 20/30: 100%|██████████| 86/86 [00:12<00:00,  6.98it/s, iou=0.896, loss=0.183] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 20 Summary:\n",
                        "Train Loss: 0.1732 | Train IoU: 0.9164\n",
                        "Val Loss: 0.2601 | Val IoU: 0.8725 | Val Acc: 0.9350\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 21/30: 100%|██████████| 86/86 [00:12<00:00,  6.91it/s, iou=0.935, loss=0.159] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 21 Summary:\n",
                        "Train Loss: 0.1713 | Train IoU: 0.9173\n",
                        "Val Loss: 0.2637 | Val IoU: 0.8737 | Val Acc: 0.9345\n",
                        "Saved Best Model! (IoU: 0.8737)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 22/30: 100%|██████████| 86/86 [00:12<00:00,  6.96it/s, iou=0.954, loss=0.105] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 22 Summary:\n",
                        "Train Loss: 0.1475 | Train IoU: 0.9280\n",
                        "Val Loss: 0.2696 | Val IoU: 0.8735 | Val Acc: 0.9347\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 23/30: 100%|██████████| 86/86 [00:12<00:00,  6.92it/s, iou=0.941, loss=0.121] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 23 Summary:\n",
                        "Train Loss: 0.1572 | Train IoU: 0.9210\n",
                        "Val Loss: 0.2534 | Val IoU: 0.8776 | Val Acc: 0.9374\n",
                        "Saved Best Model! (IoU: 0.8776)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 24/30: 100%|██████████| 86/86 [00:12<00:00,  6.96it/s, iou=0.917, loss=0.16]  \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 24 Summary:\n",
                        "Train Loss: 0.1407 | Train IoU: 0.9321\n",
                        "Val Loss: 0.2692 | Val IoU: 0.8758 | Val Acc: 0.9357\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 25/30: 100%|██████████| 86/86 [00:12<00:00,  6.92it/s, iou=0.967, loss=0.0807]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 25 Summary:\n",
                        "Train Loss: 0.1530 | Train IoU: 0.9235\n",
                        "Val Loss: 0.2642 | Val IoU: 0.8739 | Val Acc: 0.9350\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 26/30: 100%|██████████| 86/86 [00:12<00:00,  6.93it/s, iou=0.93, loss=0.124]  \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 26 Summary:\n",
                        "Train Loss: 0.1524 | Train IoU: 0.9295\n",
                        "Val Loss: 0.2767 | Val IoU: 0.8736 | Val Acc: 0.9343\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 27/30: 100%|██████████| 86/86 [00:12<00:00,  6.95it/s, iou=0.945, loss=0.104] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 27 Summary:\n",
                        "Train Loss: 0.1393 | Train IoU: 0.9301\n",
                        "Val Loss: 0.2624 | Val IoU: 0.8754 | Val Acc: 0.9368\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 28/30: 100%|██████████| 86/86 [00:12<00:00,  6.95it/s, iou=0.906, loss=0.227] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 28 Summary:\n",
                        "Train Loss: 0.1306 | Train IoU: 0.9369\n",
                        "Val Loss: 0.2669 | Val IoU: 0.8750 | Val Acc: 0.9357\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 29/30: 100%|██████████| 86/86 [00:12<00:00,  6.92it/s, iou=0.87, loss=0.262]  \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 29 Summary:\n",
                        "Train Loss: 0.1517 | Train IoU: 0.9299\n",
                        "Val Loss: 0.2796 | Val IoU: 0.8714 | Val Acc: 0.9330\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Epoch 30/30: 100%|██████████| 86/86 [00:12<00:00,  6.96it/s, iou=0.938, loss=0.155] \n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 30 Summary:\n",
                        "Train Loss: 0.1351 | Train IoU: 0.9339\n",
                        "Val Loss: 0.2712 | Val IoU: 0.8746 | Val Acc: 0.9352\n",
                        "\n",
                        "Training Complete. Best IoU: 0.8776\n"
                    ]
                }
            ],
            "source": [
                "# 7. Training Pipeline\n",
                "import glob\n",
                "import gc\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import albumentations as A\n",
                "from albumentations.pytorch import ToTensorV2\n",
                "from PIL import Image\n",
                "from torch.cuda.amp import GradScaler, autocast\n",
                "\n",
                "# Aggressive memory cleanup\n",
                "gc.collect()\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "# 7.1 Dataset Definition\n",
                "class BuildingDataset(Dataset):\n",
                "    def __init__(self, items, transforms=None):\n",
                "        self.items = items\n",
                "        self.transforms = transforms\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.items)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img_path = self.items[idx]\n",
                "        mask_path = img_path.replace(\"/images/\", \"/masks/\")\n",
                "\n",
                "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
                "        mask = np.array(Image.open(mask_path).convert(\"L\"))\n",
                "        \n",
                "        mask = (mask > 0).astype(float)\n",
                "        \n",
                "        if self.transforms:\n",
                "            augmented = self.transforms(image=image, mask=mask)\n",
                "            image = augmented['image']\n",
                "            mask = augmented['mask']\n",
                "            \n",
                "        if len(mask.shape) == 2:\n",
                "            mask = mask.unsqueeze(0)\n",
                "            \n",
                "        return image, mask\n",
                "        \n",
                "# 7.2 Augmentations - RESIZE TO 512x512 to fit in memory\n",
                "\n",
                "train_transform = A.Compose([\n",
                "    A.Resize(512, 512),  # <-- ADD THIS LINE\n",
                "    A.HorizontalFlip(p=0.5),\n",
                "    A.VerticalFlip(p=0.5),\n",
                "    A.RandomRotate90(p=0.5),\n",
                "    A.RandomBrightnessContrast(p=0.2),\n",
                "    A.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],\n",
                "        std=[0.229, 0.224, 0.225],\n",
                "    ),\n",
                "    ToTensorV2(),\n",
                "])\n",
                "val_transform = A.Compose([\n",
                "    A.Resize(512, 512),  # <-- ADD THIS LINE\n",
                "    A.Normalize(\n",
                "        mean=[0.485, 0.456, 0.406],\n",
                "        std=[0.229, 0.224, 0.225],\n",
                "    ),\n",
                "    ToTensorV2(),\n",
                "])\n",
                "\n",
                "# 7.3 Data Loaders\n",
                "all_tiles = glob.glob(f\"{IMG_DIR}/*.tif\")\n",
                "np.random.shuffle(all_tiles)\n",
                "\n",
                "split_ratio = 0.8\n",
                "split_idx = int(len(all_tiles) * split_ratio)\n",
                "\n",
                "train_files = all_tiles[:split_idx]\n",
                "val_files = all_tiles[split_idx:]\n",
                "\n",
                "print(f\"Total tiles: {len(all_tiles)}\")\n",
                "print(f\"Training: {len(train_files)}, Validation: {len(val_files)}\")\n",
                "\n",
                "train_ds = BuildingDataset(train_files, transforms=train_transform)\n",
                "val_ds = BuildingDataset(val_files, transforms=val_transform)\n",
                "\n",
                "# Use Batch Size 4 with AMP (should fit if BS=1 non-AMP failed closely)\n",
                "BATCH_SIZE = 4\n",
                "NUM_WORKERS = os.cpu_count() or 4\n",
                "\n",
                "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
                "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
                "\n",
                "# 7.4 Training Loop\n",
                "EPOCHS = 30\n",
                "LEARNING_RATE = 1e-4\n",
                "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3)\n",
                "scaler = GradScaler() # Initialize Scaler for AMP\n",
                "\n",
                "best_iou = 0.0\n",
                "save_path = \"../models/building_unet.pth\"\n",
                "os.makedirs(\"../models\", exist_ok=True)\n",
                "\n",
                "print(\"Starting training with AMP...\")\n",
                "\n",
                "for epoch in range(EPOCHS):\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    train_iou = 0\n",
                "    \n",
                "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
                "    for images, masks in loop:\n",
                "        images = images.to(DEVICE)\n",
                "        masks = masks.to(DEVICE)\n",
                "        \n",
                "        # Mixed Precision Forward\n",
                "        with autocast():\n",
                "            outputs = model(images)\n",
                "            loss = loss_fn(outputs, masks)\n",
                "        \n",
                "        # Mixed Precision Backward\n",
                "        optimizer.zero_grad()\n",
                "        scaler.scale(loss).backward()\n",
                "        scaler.step(optimizer)\n",
                "        scaler.update()\n",
                "        \n",
                "        # Metrics (detach to ensure no memory leak)\n",
                "        batch_iou, _ = calculate_metrics(outputs.detach(), masks)\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        train_iou += batch_iou\n",
                "        \n",
                "        loop.set_postfix(loss=loss.item(), iou=batch_iou)\n",
                "        \n",
                "    avg_train_loss = train_loss / len(train_loader)\n",
                "    avg_train_iou = train_iou / len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    val_iou = 0\n",
                "    val_acc = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, masks in val_loader:\n",
                "            images = images.to(DEVICE)\n",
                "            masks = masks.to(DEVICE)\n",
                "            \n",
                "            with autocast():\n",
                "                outputs = model(images)\n",
                "                loss = loss_fn(outputs, masks)\n",
                "            \n",
                "            iou, acc = calculate_metrics(outputs, masks)\n",
                "            \n",
                "            val_loss += loss.item()\n",
                "            val_iou += iou\n",
                "            val_acc += acc\n",
                "\n",
                "    avg_val_loss = val_loss / len(val_loader)\n",
                "    avg_val_iou = val_iou / len(val_loader)\n",
                "    avg_val_acc = val_acc / len(val_loader)\n",
                "    \n",
                "    scheduler.step(avg_val_iou)\n",
                "    \n",
                "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
                "    print(f\"Train Loss: {avg_train_loss:.4f} | Train IoU: {avg_train_iou:.4f}\")\n",
                "    print(f\"Val Loss: {avg_val_loss:.4f} | Val IoU: {avg_val_iou:.4f} | Val Acc: {avg_val_acc:.4f}\")\n",
                "    \n",
                "    if avg_val_iou > best_iou:\n",
                "        best_iou = avg_val_iou\n",
                "        torch.save(model.state_dict(), save_path)\n",
                "        print(f\"Saved Best Model! (IoU: {best_iou:.4f})\")\n",
                "        \n",
                "print(f\"\\nTraining Complete. Best IoU: {best_iou:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "7c32ce39",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Inference input: ../data/images/TIMMOWAL_37695_ORI.tif\n",
                        "Output directory: ../outputs\n",
                        "Model loaded successfully.\n",
                        "Starting main inference...\n",
                        "Image Dimensions: 31100x30532\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Processing Tiles: 100%|██████████| 930/930 [00:31<00:00, 29.45it/s]\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Saved CRS-preserved mask to: ../outputs/building_mask.tif\n",
                        "Saved binary mask PNG to: ../outputs/building_mask.png\n",
                        "Generating downsampled overlay...\n",
                        "Downsampling to 4095x4021\n",
                        "Saved Overlay PNG to: ../outputs/building_overlay.png\n",
                        "Done!\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import rasterio\n",
                "import numpy as np\n",
                "import torch\n",
                "import cv2\n",
                "import gc\n",
                "from rasterio.windows import Window\n",
                "from tqdm import tqdm\n",
                "from PIL import Image\n",
                "\n",
                "# 1. Configuration\n",
                "INFERENCE_IMAGE_PATH = \"../data/images/TIMMOWAL_37695_ORI.tif\"\n",
                "if not os.path.exists(INFERENCE_IMAGE_PATH):\n",
                "    import glob\n",
                "    available = glob.glob(\"../data/images/*.tif\")\n",
                "    if available:\n",
                "        INFERENCE_IMAGE_PATH = available[0]\n",
                "\n",
                "OUTPUT_DIR = \"../outputs\"\n",
                "MODEL_PATH = \"../models/building_unet.pth\"\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "TILE_SIZE = 1024\n",
                "INPUT_SIZE = 512\n",
                "\n",
                "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
                "print(f\"Inference input: {INFERENCE_IMAGE_PATH}\")\n",
                "print(f\"Output directory: {OUTPUT_DIR}\")\n",
                "\n",
                "# 2. Load Model\n",
                "import segmentation_models_pytorch as smp\n",
                "model = smp.Unet(\n",
                "    encoder_name=\"resnet34\",\n",
                "    encoder_weights=None, \n",
                "    in_channels=3,\n",
                "    classes=1,\n",
                ")\n",
                "if os.path.exists(MODEL_PATH):\n",
                "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
                "    model.to(DEVICE)\n",
                "    model.eval()\n",
                "    print(\"Model loaded successfully.\")\n",
                "else:\n",
                "    raise FileNotFoundError(f\"Model not found at {MODEL_PATH}.\")\n",
                "\n",
                "# 3. Inference Function\n",
                "def predict_large_image(image_path, model, tile_size, input_size, device):\n",
                "    with rasterio.open(image_path) as src:\n",
                "        h, w = src.height, src.width\n",
                "        profile = src.profile.copy()\n",
                "        full_mask = np.zeros((h, w), dtype=np.uint8)\n",
                "        print(f\"Image Dimensions: {w}x{h}\")\n",
                "        \n",
                "        x_steps = list(range(0, w, tile_size))\n",
                "        y_steps = list(range(0, h, tile_size))\n",
                "        total_tiles = len(x_steps) * len(y_steps)\n",
                "        \n",
                "        with tqdm(total=total_tiles, desc=\"Processing Tiles\") as pbar:\n",
                "            for row in y_steps:\n",
                "                for col in x_steps:\n",
                "                    window = Window(col, row, min(tile_size, w - col), min(tile_size, h - row))\n",
                "                    img_data = src.read((1, 2, 3), window=window)\n",
                "                    curr_h, curr_w = img_data.shape[1], img_data.shape[2]\n",
                "                    \n",
                "                    img_np = img_data.transpose(1, 2, 0)\n",
                "                    img_resized = cv2.resize(img_np, (input_size, input_size))\n",
                "                    \n",
                "                    mean = np.array([0.485, 0.456, 0.406])\n",
                "                    std = np.array([0.229, 0.224, 0.225])\n",
                "                    img_norm = (img_resized / 255.0 - mean) / std\n",
                "                    \n",
                "                    img_t = torch.from_numpy(img_norm.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n",
                "                    \n",
                "                    with torch.no_grad():\n",
                "                        with torch.amp.autocast('cuda'):\n",
                "                            output = model(img_t)\n",
                "                            output_mask = torch.sigmoid(output).cpu().numpy()[0, 0].astype(np.float32)\n",
                "                    \n",
                "                    mask_resized = cv2.resize(output_mask, (curr_w, curr_h))\n",
                "                    binary_mask = (mask_resized > 0.5).astype(np.uint8)\n",
                "                    full_mask[row:row+curr_h, col:col+curr_w] = binary_mask\n",
                "                    pbar.update(1)\n",
                "    return full_mask, profile\n",
                "\n",
                "# 4. Run Prediction\n",
                "print(\"Starting main inference...\")\n",
                "predicted_mask, source_profile = predict_large_image(INFERENCE_IMAGE_PATH, model, TILE_SIZE, INPUT_SIZE, DEVICE)\n",
                "\n",
                "# 5.1 Save as GeoTIFF\n",
                "tif_output_path = os.path.join(OUTPUT_DIR, \"building_mask.tif\")\n",
                "source_profile.update(dtype=rasterio.uint8, count=1, compress='lzw')\n",
                "with rasterio.open(tif_output_path, 'w', **source_profile) as dst:\n",
                "    dst.write(predicted_mask, 1)\n",
                "print(f\"Saved CRS-preserved mask to: {tif_output_path}\")\n",
                "\n",
                "# 5.2 Save as PNG\n",
                "png_output_path = os.path.join(OUTPUT_DIR, \"building_mask.png\")\n",
                "Image.fromarray(predicted_mask * 255).save(png_output_path)\n",
                "print(f\"Saved binary mask PNG to: {png_output_path}\")\n",
                "\n",
                "# 5.3 Create Downsampled Overlay (Memory-Efficient)\n",
                "overlay_path = os.path.join(OUTPUT_DIR, \"building_overlay.png\")\n",
                "print(\"Generating downsampled overlay...\")\n",
                "\n",
                "MAX_DIM = 4096\n",
                "with rasterio.open(INFERENCE_IMAGE_PATH) as src:\n",
                "    h, w = src.height, src.width\n",
                "    scale = min(MAX_DIM / w, MAX_DIM / h, 1.0)\n",
                "    new_w, new_h = int(w * scale), int(h * scale)\n",
                "    print(f\"Downsampling to {new_w}x{new_h}\")\n",
                "    image_rgb = src.read((1, 2, 3), out_shape=(3, new_h, new_w),\n",
                "                         resampling=rasterio.enums.Resampling.bilinear).transpose(1, 2, 0)\n",
                "\n",
                "mask_small = cv2.resize(predicted_mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
                "overlay_img = image_rgb.copy()\n",
                "red_color = np.zeros_like(overlay_img)\n",
                "red_color[:, :, 0] = 255\n",
                "alpha = 0.5\n",
                "mask_indices = mask_small == 1\n",
                "overlay_img[mask_indices] = (alpha * overlay_img[mask_indices] + (1 - alpha) * red_color[mask_indices]).astype(np.uint8)\n",
                "Image.fromarray(overlay_img).save(overlay_path)\n",
                "print(f\"Saved Overlay PNG to: {overlay_path}\")\n",
                "\n",
                "del image_rgb, overlay_img, mask_small, red_color, predicted_mask\n",
                "gc.collect()\n",
                "print(\"Done!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "1f017d74",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading predicted mask...\n"
                    ]
                },
                {
                    "ename": "DecompressionBombError",
                    "evalue": "Image size (949545200 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack.",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mDecompressionBombError\u001b[39m                    Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 1. Load Predicted Mask\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading predicted mask...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m mask = np.array(\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMASK_INPUT_PATH\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mL\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     17\u001b[39m mask_binary = (mask > \u001b[32m127\u001b[39m).astype(np.uint8)  \u001b[38;5;66;03m# Ensure binary\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal mask shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmask_binary.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/natgeo/venv/lib/python3.13/site-packages/PIL/Image.py:3539\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3536\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   3537\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3539\u001b[39m im = \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3541\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[32m   3542\u001b[39m     checked_formats = ID.copy()\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/natgeo/venv/lib/python3.13/site-packages/PIL/Image.py:3528\u001b[39m, in \u001b[36mopen.<locals>._open_core\u001b[39m\u001b[34m(fp, filename, prefix, formats)\u001b[39m\n\u001b[32m   3526\u001b[39m         fp.seek(\u001b[32m0\u001b[39m)\n\u001b[32m   3527\u001b[39m         im = factory(fp, filename)\n\u001b[32m-> \u001b[39m\u001b[32m3528\u001b[39m         \u001b[43m_decompression_bomb_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3529\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[32m   3530\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mSyntaxError\u001b[39;00m, \u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, struct.error) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Projects/natgeo/venv/lib/python3.13/site-packages/PIL/Image.py:3429\u001b[39m, in \u001b[36m_decompression_bomb_check\u001b[39m\u001b[34m(size)\u001b[39m\n\u001b[32m   3424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixels > \u001b[32m2\u001b[39m * MAX_IMAGE_PIXELS:\n\u001b[32m   3425\u001b[39m     msg = (\n\u001b[32m   3426\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m2\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3427\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpixels, could be decompression bomb DOS attack.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3428\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3429\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DecompressionBombError(msg)\n\u001b[32m   3431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pixels > MAX_IMAGE_PIXELS:\n\u001b[32m   3432\u001b[39m     warnings.warn(\n\u001b[32m   3433\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpixels\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixels) exceeds limit of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMAX_IMAGE_PIXELS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pixels, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3434\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcould be decompression bomb DOS attack.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3435\u001b[39m         DecompressionBombWarning,\n\u001b[32m   3436\u001b[39m     )\n",
                        "\u001b[31mDecompressionBombError\u001b[39m: Image size (949545200 pixels) exceeds limit of 178956970 pixels, could be decompression bomb DOS attack."
                    ]
                }
            ],
            "source": [
                "import cv2\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import matplotlib.pyplot as plt\n",
                "from scipy import ndimage\n",
                "import os\n",
                "\n",
                "# Configuration\n",
                "MASK_INPUT_PATH = \"../outputs/utility_mask.png\"\n",
                "MASK_OUTPUT_PATH = \"../outputs/utility_mask_cleaned.png\"\n",
                "OVERLAY_OUTPUT_PATH = \"../outputs/utility_overlay_final.png\"\n",
                "MIN_COMPONENT_SIZE = 50  # pixels\n",
                "\n",
                "# 1. Load Predicted Mask\n",
                "print(\"Loading predicted mask...\")\n",
                "mask = np.array(Image.open(MASK_INPUT_PATH).convert('L'))\n",
                "mask_binary = (mask > 127).astype(np.uint8)  # Ensure binary\n",
                "\n",
                "print(f\"Original mask shape: {mask_binary.shape}\")\n",
                "print(f\"Original utility pixels: {mask_binary.sum()}\")\n",
                "\n",
                "# 2. Morphological Opening (Remove Noise)\n",
                "print(\"\\n1. Applying morphological opening...\")\n",
                "# Kernel for opening (removes small bright spots/noise)\n",
                "kernel_open = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
                "mask_opened = cv2.morphologyEx(mask_binary, cv2.MORPH_OPEN, kernel_open, iterations=1)\n",
                "\n",
                "print(f\"   After opening: {mask_opened.sum()} pixels\")\n",
                "\n",
                "# 3. Remove Small Components\n",
                "print(f\"\\n2. Removing components smaller than {MIN_COMPONENT_SIZE} pixels...\")\n",
                "# Label connected components\n",
                "num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n",
                "    mask_opened, connectivity=8\n",
                ")\n",
                "\n",
                "# Filter by size\n",
                "mask_filtered = np.zeros_like(mask_opened)\n",
                "for i in range(1, num_labels):  # Skip background (label 0)\n",
                "    area = stats[i, cv2.CC_STAT_AREA]\n",
                "    if area >= MIN_COMPONENT_SIZE:\n",
                "        mask_filtered[labels == i] = 1\n",
                "\n",
                "print(f\"   Components before: {num_labels - 1}\")\n",
                "print(f\"   Components after: {(mask_filtered > 0).sum() > 0}\")\n",
                "print(f\"   Pixels remaining: {mask_filtered.sum()}\")\n",
                "\n",
                "# 4. Morphological Closing (Smooth Boundaries & Fill Gaps)\n",
                "print(\"\\n3. Smoothing boundaries...\")\n",
                "# Closing smooths object contours and closes small holes\n",
                "kernel_close = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
                "mask_cleaned = cv2.morphologyEx(mask_filtered, cv2.MORPH_CLOSE, kernel_close, iterations=1)\n",
                "\n",
                "print(f\"   Final utility pixels: {mask_cleaned.sum()}\")\n",
                "\n",
                "# 5. Save Cleaned Mask\n",
                "print(f\"\\n✓ Saving cleaned mask to: {MASK_OUTPUT_PATH}\")\n",
                "Image.fromarray(mask_cleaned * 255).save(MASK_OUTPUT_PATH)\n",
                "\n",
                "# 6. Create Final Overlay (Downsampled for presentation)\n",
                "print(\"\\nCreating presentation-quality overlay...\")\n",
                "\n",
                "# Load original image (downsampled)\n",
                "INFERENCE_IMAGE_PATH = \"../data/images/TIMMOWAL_37695_ORI.tif\"\n",
                "MAX_DIM = 4096\n",
                "\n",
                "import rasterio\n",
                "with rasterio.open(INFERENCE_IMAGE_PATH) as src:\n",
                "    h, w = src.height, src.width\n",
                "    scale = min(MAX_DIM / w, MAX_DIM / h, 1.0)\n",
                "    new_w, new_h = int(w * scale), int(h * scale)\n",
                "    \n",
                "    print(f\"   Overlay size: {new_w}x{new_h}\")\n",
                "    \n",
                "    image_rgb = src.read(\n",
                "        (1, 2, 3),\n",
                "        out_shape=(3, new_h, new_w),\n",
                "        resampling=rasterio.enums.Resampling.bilinear\n",
                "    ).transpose(1, 2, 0)\n",
                "\n",
                "# Resize cleaned mask to match\n",
                "mask_display = cv2.resize(mask_cleaned, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
                "\n",
                "# Create professional overlay with semi-transparent utilities in cyan/blue\n",
                "overlay_img = image_rgb.copy()\n",
                "# Use cyan for better visibility on aerial imagery\n",
                "overlay_color = np.array([0, 255, 255])  # Cyan in RGB\n",
                "\n",
                "# Create colored mask\n",
                "color_mask = np.zeros_like(image_rgb)\n",
                "color_mask[mask_display == 1] = overlay_color\n",
                "\n",
                "# Blend with alpha\n",
                "alpha = 0.6\n",
                "final_overlay = (alpha * image_rgb + (1 - alpha) * color_mask).astype(np.uint8)\n",
                "# Only apply where mask is present\n",
                "final_overlay[mask_display == 0] = image_rgb[mask_display == 0]\n",
                "\n",
                "# Save\n",
                "Image.fromarray(final_overlay).save(OVERLAY_OUTPUT_PATH)\n",
                "print(f\"✓ Saved final overlay to: {OVERLAY_OUTPUT_PATH}\")\n",
                "\n",
                "# 7. Presentation Visualization\n",
                "print(\"\\nGenerating comparison visualization...\")\n",
                "\n",
                "fig = plt.figure(figsize=(20, 12))\n",
                "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.2)\n",
                "\n",
                "# Row 1: Processing stages\n",
                "ax1 = fig.add_subplot(gs[0, 0])\n",
                "ax1.imshow(mask_binary, cmap='gray')\n",
                "ax1.set_title('1. Original Prediction', fontsize=14, fontweight='bold')\n",
                "ax1.axis('off')\n",
                "\n",
                "ax2 = fig.add_subplot(gs[0, 1])\n",
                "ax2.imshow(mask_opened, cmap='gray')\n",
                "ax2.set_title('2. After Opening (Noise Removal)', fontsize=14, fontweight='bold')\n",
                "ax2.axis('off')\n",
                "\n",
                "ax3 = fig.add_subplot(gs[0, 2])\n",
                "ax3.imshow(mask_filtered, cmap='gray')\n",
                "ax3.set_title(f'3. After Size Filter (>{MIN_COMPONENT_SIZE}px)', fontsize=14, fontweight='bold')\n",
                "ax3.axis('off')\n",
                "\n",
                "# Row 2: Final results\n",
                "ax4 = fig.add_subplot(gs[1, 0])\n",
                "ax4.imshow(mask_cleaned, cmap='gray')\n",
                "ax4.set_title('4. Final Cleaned Mask', fontsize=14, fontweight='bold')\n",
                "ax4.axis('off')\n",
                "\n",
                "ax5 = fig.add_subplot(gs[1, 1:])\n",
                "ax5.imshow(final_overlay)\n",
                "ax5.set_title('Final Overlay - Utilities in Cyan', fontsize=14, fontweight='bold')\n",
                "ax5.axis('off')\n",
                "\n",
                "# Add statistics box\n",
                "stats_text = f\"\"\"\n",
                "Post-Processing Statistics:\n",
                "━━━━━━━━━━━━━━━━━━━━━━\n",
                "Original pixels:    {mask_binary.sum():,}\n",
                "After opening:      {mask_opened.sum():,}\n",
                "After filtering:    {mask_filtered.sum():,}\n",
                "Final cleaned:      {mask_cleaned.sum():,}\n",
                "\n",
                "Noise removed:      {mask_binary.sum() - mask_cleaned.sum():,} pixels\n",
                "Reduction:          {((mask_binary.sum() - mask_cleaned.sum()) / max(mask_binary.sum(), 1) * 100):.1f}%\n",
                "\"\"\"\n",
                "\n",
                "fig.text(0.02, 0.02, stats_text, fontsize=11, family='monospace',\n",
                "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
                "\n",
                "plt.suptitle('Utility Segmentation - Post-Processing Pipeline', \n",
                "             fontsize=16, fontweight='bold', y=0.98)\n",
                "\n",
                "# Save figure\n",
                "comparison_path = \"../outputs/utility_postprocessing_comparison.png\"\n",
                "plt.savefig(comparison_path, dpi=150, bbox_inches='tight')\n",
                "print(f\"✓ Saved comparison to: {comparison_path}\")\n",
                "\n",
                "plt.show()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"Post-processing Complete!\")\n",
                "print(\"=\"*60)\n",
                "print(f\"\\nOutput files:\")\n",
                "print(f\"  • Cleaned mask:  {MASK_OUTPUT_PATH}\")\n",
                "print(f\"  • Final overlay: {OVERLAY_OUTPUT_PATH}\")\n",
                "print(f\"  • Comparison:    {comparison_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "32e9d566",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
